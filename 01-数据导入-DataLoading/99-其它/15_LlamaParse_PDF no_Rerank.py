from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import VectorStoreIndex
from llama_index.core import Settings

from dotenv import load_dotenv
load_dotenv()   

import os
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.deepseek.base import DeepSeek
import time

# API access to llama-cloud
os.environ["LLAMA_CLOUD_API_KEY"] = "llx-9LWXKaFjF7DgO3clPLL70nxb3sfngHXHbf5bZpnZSFvGNkcp"

# Setup LLM and embedding models
model_path = "/root/autodl-tmp/huggingface_cache/hub/models--BAAI--bge-m3"
embed_model = HuggingFaceEmbedding(
    model_name=model_path,
    # 对于本地模型，通常不需要指定 device 或 local_files_only，
    # 但为了与 LangChain 代码保持一致且确保本地加载，可以保留
    model_kwargs={
        "device": "cpu",
        "local_files_only": True
    }
)
llm = DeepSeek(
    api_key=os.getenv("DEEPSEEK_API_KEY"),  # 从环境变量获取 API 密钥
    model="deepseek-chat",  # 使用 deepseek-chat 模型
    temperature=0,
    max_tokens=1000,
    timeout=60  # 设置超时时间为60秒
)

Settings.llm = llm
Settings.embed_model = embed_model

# LlamaParse PDF reader for PDF Parsing
from llama_parse import LlamaParse

# Assuming the file path is correct now based on previous fixes.
pdf_path = "/root/autodl-tmp/rag-in-action/90-文档-Data/复杂PDF/uber_10q_march_2022.pdf"

print(f"Using LlamaParse to load data from: {pdf_path}")
parser = LlamaParse(api_key=os.getenv("LLAMA_CLOUD_API_KEY"), result_type='markdown')
documents = parser.load_data(pdf_path)
print(f"LlamaParse loaded {len(documents)} documents.")

# Check if documents were loaded successfully
if not documents:
    print("Error: LlamaParse returned no documents. Please check the file or Llama Cloud status.")
    # Exit or handle the error appropriately
    exit()

print("Proceeding to node parsing...")

from llama_index.core.node_parser import SentenceSplitter

# Parsing documents into nodes using SentenceSplitter...
# Using SentenceSplitter for simpler parsing to diagnose potential issues with MarkdownElementNodeParser
print("Parsing documents into nodes using SentenceSplitter...")
node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=100)

nodes = node_parser.get_nodes_from_documents(documents)
print(f"Parsed {len(nodes)} nodes using SentenceSplitter.")

# Check if nodes were created
if not nodes:
     print("Error: SentenceSplitter created no nodes.")
     exit()

# With SentenceSplitter, we don't typically separate into text_nodes and index_nodes in this way.
# The nodes generated by SentenceSplitter are usually the final nodes.
# Let's proceed with creating a single index from these nodes.

# Based on the original script structure and the subsequent index creation,
# it seems the original script intended to use text_nodes for recursive index and raw_index.
# Let's adapt to use the nodes from SentenceSplitter for both.

text_nodes = nodes # Use nodes from SentenceSplitter as text_nodes
index_nodes = [] # No separate index nodes with this simple splitter

print(f"Using nodes from SentenceSplitter for indexing. Total nodes: {len(text_nodes)}.")

# Proceed with indexing and querying if text_nodes are available
print("Creating vector index (using nodes from SentenceSplitter)...")
# Use only text_nodes (which are the nodes from SentenceSplitter) for the main index
recursive_index = VectorStoreIndex(nodes=text_nodes)
print("Vector index created.")

# Original script also created a raw_index from documents. Let's keep that part.
print("Creating raw index (from original documents)...")
raw_index = VectorStoreIndex.from_documents(documents)
print("Raw index created.")

print("Creating query engines...")
# Use the index created from SentenceSplitter nodes for the recursive query engine
recursive_query_engine = recursive_index.as_query_engine(
    similarity_top_k=15,  verbose=True
)

# Use the raw index from original documents for the raw query engine
raw_query_engine = raw_index.as_query_engine(
    similarity_top_k=15,
)
print("Query engines created.")

# Example query
query = "What is the change of free cash flow and what is the rate from the financial and operational highlights?"

print(f"\nExecuting query using raw query engine: {query}")
start_time_raw = time.time()
response_1 = raw_query_engine.query(query)
end_time_raw = time.time()
print(f"Raw query engine finished in {end_time_raw - start_time_raw:.2f} seconds.")
print("\n************New LlamaParse+ Basic Query Engine************")
print(response_1)

print(f"\nExecuting query using recursive query engine: {query}")
start_time_recursive = time.time()
response_2 = recursive_query_engine.query(query)
end_time_recursive = time.time()
print(f"Recursive query engine finished in {end_time_recursive - start_time_recursive:.2f} seconds.")
print(
    "\n************New LlamaParse+ Recursive Retriever Query Engine************"
)
print(response_2)

print("\nScript finished successfully (or reached an earlier exit).")